\documentclass[12pt]{article}
\usepackage{url}
\usepackage{dirtytalk}
\usepackage[margin=1.1in]{geometry}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{listings}
\usepackage{color}
\usepackage[autostyle]{csquotes} 
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{float}

\newcommand\todo[1]{\textcolor{red}{#1}}

\urlstyle{same}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\lstset{language=Java}

\begin{document}

\vspace*{\fill}
\centerline{\large{\textbf{Abstract}}}
Machine learning methods require many workers, all of whom iteratively analyse a dataset in parallel in a series of rounds, collecting their progress centrally at the end of each round. By implementing a work stealing algorithm, the delay introduced by the slowest workers each round will be mitigated. The goal of this project is to implement a work stealing algorithm and validate that it does provide a throughput increase of rounds completed per second. The algorithm is validated on the MNIST trainging set and is built on top of the work of the LSDS group at Imperial College London. An analysis and evaluation will be discussed in this report.
\vspace*{\fill}

\newpage

\tableofcontents

\newpage

\section{Chapter 1 - Introduction}

\subsection{Motivation}

The volume of data produced per year is increasing exponentially through social media, embedded Internet of Things sensors, and even financial records. Analysis of this data is incredibly valuable to the data owners: email services can better detect spam, banks can monitor for fraud, and cities can be managed more efficiently.
\newline
\newline
In a report by The Department for Business Innovation and Skills\cite{smartCities}, it was estimated that the \say{global market for smart urban systems for transport, energy,
healthcare, water and waste will amount to around \$400 Billion pa. by 2020}. Through more intelligent city management and improved data collection \say{we can manage cities more effectively} and \say{anticipate and solve problems more cost effectively}.
\newline
\newline
In order to achieve these goals of cost effective and efficient public services the data collected must be analysed. Through machine learning techniques, automated systems can ensure continuous monitoring of services such as traffic and crime, and anticipate future events based on past data.
\newline
\newline
Machine learning on large data sets has been common in the private sector for some while now. Google use emails to detect and prevent spam, Amazon recommend products to purchase based on a customer's history and other customers making similar purchases, and Facebook use facial recognition software to automatically ``tag'' friends in photos.
\newline
\newline
At a high level of abstraction, these machine learning models work by ingesting large volumes of data, detecting patterns, and then mapping further inputs to outputs. For example, when a picture containing a face is uploaded to Facebook they will attempt to map it to users they believe to be in the picture. The initial process of ingesting the data however can be incredibly time consuming, Facebook reported in 2010\cite{facebookHaystack} that they stored 260 billion images representing over 20PB of data.
\newline
\newline
In order to speed the process of reading and analysing data of this size, the process is sharded between many workers. Each worker analyses a subset of the data and aggregating their results in a central model in a process called training. As additional data is received, there will be an attempt to classify them against the central model which will assign a label identifying the data. The sharding of data presents new challenges in an attempt to reduce training times and simultaneously improving the accuracy of data labelling.
\newline
\newline
One of the major problems with any sharded system is the ability to handle slow workers, also known as stragglers. Many algorithms in the field of machine learning are synchronous and require the termination of all workers before proceeding to the next round of computation and are therefore limited by the slowest worker. If the effects of these stragglers were to be alleviated, it would improve the performance of the algorithm and thus complete the training phase sooner.  

\newpage

\subsection{Problem Statement}
There are currently many Machine Learning frameworks used in industry, and as with most industrial software, performance is a major concern. These frameworks are typically able to run in a distributed fashion across many workers in order to improve performance. In any distributed environment, there will be a distribution of execution run times, caused by non-deterministic factors such as kernel scheduling, network packet drops, and cache misses. In some pathological cases there will be workers who are significantly delayed compared to other worker. These workers are referred to as stragglers. Certain machine learning algorithms are dependent on the slowest workers and will block until completion of the slowest straggler.
\newline
Workers can be delayed for both transient reasons such as temporary network degradation or resource over subscription, but also for permanent reasons such as disk drive failures. This project will focus on mitigating the effects of transient failures. If a worker experiences a permanent failure, the larger task can either be aborted, or work can be redistributed amongst other, healthy workers. In an initial revision of Google's MapReduce \citep{dean2008mapreduce}, the authors write that "given that there is only a single master, its failure is unlikely; therefore our current implementation aborts the MapReduce computation if the master fails". A solution used by many distributed frameworks such as MapReduce and Apache Spark \cite{zaharia2012resilient} is to "re-run it on another node".
\newline
These solutions to permanent worker failures have been very successful and as such the problem of failing workers is solved in industry. This project will focus on temporary performance degradation of workers, especially so in an environment of limited physical resources. Forward progress will be made using the workers, however they will operate at their maximum capacity and will not impair other workers.
\newline
The aim of this project is to develop and evaluate algorithms to mitigate the effects of these slowest stragglers through the use of work-stealing. We also aim to develop such an algorithm that does not negatively affect the accuracy of the system, as this is the primary objective, with performance being secondary.

\newpage

\subsection{Objective}
The objective of this project are:

\begin{enumerate}
\item Evaluate the performance improvements through the use of dynamically scheduled threads rather than statically scheduled.
\item Develop a centralised and decentralised algorithm so that faster workers may steal work from slower workers.
\item Demonstrate the effect of these two algorithms in both the presence and absence of stragglers.
\end{enumerate}

\newpage

\section{Chapter 2 - Background}

\subsection{Work Stealing}

In a system comprised of many independent workers each of which have a set of tasks which they are reponsible for computing, there will be some variance in the rate of task completions among workers.
\newline
\newline
The principle behind work stealing is to increase the total utilisation of resources in a processing system by exchanging tasks between workers according to their usage. Each worker maintains a queue of tasks to run, and when a worker has completed its tasks it will ``steal'' tasks from another worker's queue.
\newline
\newline
The Java Runtime Environment contains the ForkJoinPool\cite{javaThreads}, a convenient concurrency primitive where tasks can be submitted to a pool of workers and executed in parallel. Each worker maintains two separate task queues, one for work it has been allocated, and another for work that it has stolen. Tasks that are submitted to the worker pool can optionally be written so that they are able to be decomposed into smaller tasks, which can be divided among workers.
\newline

\IncMargin{1em}
\begin{algorithm}[H]
  \eIf{my portion of the work is small enough}{
   do the work directly\;
   }{
   split my work into two pieces\;
   invoke the two pieces and wait for the results\;
  }
 \caption{ForkJoin Algorithm}
 \label{ForkJoinAlgorithm}
\end{algorithm}
\DecMargin{1em}
\medskip
Algorithm \ref{ForkJoinAlgorithm} contains the pseudocode used in the Java language documentation to explain the ForkJoinPool framework. The ``fork'' occurs on line 4 where a task is divided into two subtasks, and the ``join'' is line 5 when waiting for the results.

\newpage

\begin{lstlisting}[caption={An implementation of the Fibonacci algorithm using the ForkJoinFramework taken from the Java documentation},label=JavaRecursive]
public class Fibonacci extends RecursiveTask<Integer> {
  final int n;
  Fibonacci(int n) { this.n = n; }
  Integer compute() {
    if (n <= 1)
      return n;
    Fibonacci f1 = new Fibonacci(n - 1);
    f1.fork();
    Fibonacci f2 = new Fibonacci(n - 2);
    return f2.compute() + f1.join();
  }
}
\end{lstlisting}



It is important to note that work stealing occurs at both a user-level and kernel-level on a modern operating system. However, each of these has a different use case for work stealing. In the above Java example, a user-level program, the use of a ForkJoinPool was to convert a serial program into a parallel equivalent with a decreased runtime. Conversely, the Linux scheduler, a kernel-level program, uses work stealing to ensure fair access to resources for users. The programs presented in this paper are user-level programs and as such they are focused on improving performance.

\subsubsection{Dynamic vs Static Scheduling}
In the area of parallel computing, there exists the concept of an "embarrassingly parallel" task, which involves a set of operations with no inter-operation dependencies. A scalar-vector product is such a task because each element of the vector can be multiplied independently. Embarrassingly parallel tasks are limited by the computational resources available until an upper bound is met. The upper bound for a scalar-vector product is the number of elements in the vector. Beyond this amount, no additional resources will be useful as there is no work available for them to execute.
\newline
We shall now consider the case in which there is a pool of work to complete, and fewer computational resources than the amount of work, implying that each resource will be required to execute at least one piece of work. In such a model we can assign work to these resources with either a dynamic or static schedule.
\newline
A static method is one where resources are assigned work ahead of time, usually using some combination of resource identifiers and work identifiers. An example of a statically scheduled algorithm of a scalar-vector product is presented in Algorithm \ref{StaticSchedule}.

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{A reference $N$ to a vector of size $n$}
 \Input{The scalar $s$}
 \Input{This resource's identifier $i$}
 \Input{The number of resources $r$}
 \BlankLine
 
 $j = i$\;
 \While{$j < n$}{
   $N[j] \longleftarrow N[j]*s$\;
   $j \longleftarrow j + r$\;
 }
 \caption{Statically Scheduled Scalar-Vector Product}
 \label{StaticSchedule}
\end{algorithm}
\DecMargin{1em}
\medskip

The above algorithm is conceptually very simple, and is guaranteed to eventually multiply every element in the vector with the given scalar in the absence of resource failures. 
\newline
However, under this schedule it is possible for the performance of the overall algorithm to be impacted by a single, slower resource. Although all resources operate independently the overall completion of the algorithm is dependent on the slowest result being completed. 
\newline
Some optimisations may include grouping together the elements that a resource will need to access to improve cache performance, but these are inconsequential to the problem of dealing with the slowest resource.
\newline
However, what if instead of resources being assigned work ahead of time, they were able to complete work at the fastest rate of which they are capable. This would alleviate the impact caused by the slowest worker simply due to it processing fewer tasks.
\newline
\newline
Dynamic scheduling usually requires the use of some piece of state to track progress, and resources modify the state to represent work completion. These pieces of state are usually atomic or thread-safe data structures in order to avoid the risk of race conditions.
\newline
Continuing the above example of a scalar-vector product Algorithm \ref{DynamicSchedule} presents an alternative to Algorithm \ref{StaticSchedule} which is more resilient against slow resources.
\newline

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{The vector $N$ of size $n$}
 \Input{The scalar $s$}
 \Input{The number of resources $r$}
 \Input{An integer $c$ to represent the current element being processed, and that can be incremented atomically}
 \BlankLine
 
 $j = 0$\;
 \While{$(j \leftarrow c.getAndInc()) < n$}{
   $N[j] \longleftarrow N[j]*s$\;
 }
 \caption{Dynamically Scheduled Scalar-Vector Product}
 \label{DynamicSchedule}
\end{algorithm}
\DecMargin{1em}
\medskip

In Algorithm \ref{DynamicSchedule} the variable $j$ is a thread-local variable used to store the return value of the $getAndInc$ operation on the atomic integer which atomically returns the current value and incrementing it. Because the value of $c$ only monotonically and atomically increases, it is not possible for multiple threads to have the same value of $c$, and therefore each element of $N$ will only be modified once. It can be seen that the rate of completion of such an algorithm is now less vulnerable to the slowest resource as faster resources can effectively "steal" some of the work the slower resource would have been allocated under a static schedule.
\newline
If the relative performances of the resources were known ahead of time, it would be possible to construct a static scheduling which would take account of this and attempt to weight the amount of work a resource receives by its performance. However, such an algorithm would be unable to deal with dynamic variations in performance often encountered in real world use cases. There are many non-deterministic factors affecting the performance of the overall algorithm and a static scheduler is not flexible enough to adapt to consider these factors when assigning work.
\newline
It is also important to realise that not all algorithms could benefit from a dynamic scheduling. If the workers are co-located on a single, physical machine then the average case for inter-resource communication will be substantially lower than for inter-machine communication. In the above example, the atomically incrementable variable could be accessed over a network connection, but the overhead of such an exchange would likely outweigh any performance benefits of work-stealing for most application executions.
\newline
There are pathological cases in which different schedules provide no benefit. Consider a resource that has crashed in an environment with no upper bound on execution time of a task. Other workers will be unable to determine if a worker has either crashed or is currently processing.
\newline
Using the definition of a synchronous algorithm from \cite{mageeanalyzing}.

\blockquote{Synchronous distributed systems are those in which there is assumed
to be a known upper bound on each processing step, a known upper bound on
message transmission and processes have perfectly synchronized physical
clocks.}

From this definition it can be seen that both Algorithm \ref{StaticSchedule} and Algorithm \ref{DynamicSchedule} are not synchronous as there is no bound on processing steps, and it cannot be assumed that all workers have synchronised physical clocks, therefore stragglers appear. The problem is handling the case where a single resource has either crashed, or could be taking an indefinite amount of time to process. In the case where execution is blocked on a single node for an arbitrary amount of time, no scheduling system will be of use.

\subsubsection{Side Effect Free Operations and Idempotence}
Algorithms \ref{StaticSchedule} and \ref{DynamicSchedule} are both stateful solutions to the problem of a scalar vector product. This is because they are assumed to be run on multiple resources who will modify some global piece of state, in this case the vector.
\newline
We will instead consider an alternative solution which decouples the computation from the state modification. A set of $r$ resources shall be divided into a single, central resource that will manage updates to the input vector, and $r-1$ resources responsible for computing the product of a scalar and vector element.

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{The vector $N$ of size $n$}
 \Input{The scalar $s$}
 \Input{The number of resources $r$, $r < n$}
 \Output{The vector $N$ after a scalar-vector product with $s$}
 \BlankLine
 
 $j = 0$\;
 \For{$i \gets 1$ \KwTo $r$}{
   $sendTaskToResource(i, multiply, N[j], s)$\;
   $j \longleftarrow j + 1$\;
 }
 \While{$true$}{
   $index, result, resourceID = fetchLatestResult()$\;
   $N[index] \longleftarrow result$\;
   \If{$j = n - 1$}{
     $break$\;
   }
   $sendTaskToResource(resourceID, multiply, N[j], s)$\;
   $j \longleftarrow j + 1$\;
 }
 \caption{Central resource responsible for state manipulation}
 \label{StatelessProduct}
\end{algorithm}
\DecMargin{1em}
\medskip

The function $sendTaskToResource$ used in Algorithm \ref{StatelessProduct} instructs the resource with the identifier equal to the first parameter to run the task identified by the second parameter using all subsequent parameters as task arguments.
\newline
In this example the other resources will be executing a simple multiplication of their inputs: the vector element and the scalar. Note that the actual computation here is very simply purely for demonstrative purposes. In a real-world application, the work would be much more intensive and justify the need for multiple workers. This multiplication is also a function as it is guaranteed that given the same inputs that the same output will be returned.
\newline
The benefit of this system is that worker computations are now side-effect free as they only operate on state passed in as a parameter, rather than any global state. If the pathological case previously discussed where a single worker spends a potentially unbounded amount of time were to occur it can now be mitigated. The solution would be to wait until all work has been assigned and then for all outstanding tasks issue a second request to a second resource and the central resource will use whichever value returns soonest. This is only possible because the operations executed on each resource are side-effect free functions which guarantee that they will provide the same output give identical inputs.
\newline
Such a solution is not viable without creating side effect free tasks as if two executions of the task were to occur they could both modify the external state.
\newline
Consider Figure \ref{OverlappingExecution} which visualises how the overlapping of two executions could result in an incorrect result. There are solutions which account for this, however, it is much simpler to develop a solution for which this problem cannot occur. 

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{A demonstration of how multiple executions of the same task can interleave to return an incorrect result}
  \label{OverlappingExecution}
\end{figure}

In Algorithm \ref{StatelessProduct} the only state modification is assignment which is an idempotent operation meaning that it can occur multiple times and still return an identical result. Therefore if two resources were to return their identical results before the termination of the algorithm it would not cause the result to be incorrect.
\newline
This approach has been used in industry with a notable example being Google's MapReduce framework. The white paper released detailing the framework \cite{dean2008mapreduce} contained the following explanation of the framework's solution to dealing with stragglers.

\blockquote{When a MapReduce operation is close
to completion, the master schedules backup executions
of the remaining in-progress tasks. The task is marked
as completed whenever either the primary or the backup
execution completes.}

\newpage

\subsection{Machine Learning}
Machine Learning is a field of Computer Science associated with developing algorithms that can learn from and make predictions from data\cite{machineLearningDef}. It is closely related to the fields of Computational Statistics and Mathematical Optimisations, and results in computers which are able to detect patterns in data without explicit programming. The use of machine learning has spread well into industry and is now commonly used in areas such as finance, medicine, and telecommunications.
\newline
\subsubsection{Supervised Learning}
This work will be focused on an area of machine learning called \textbf{Supervised Learning}. In supervised learning each input datum to the algorithm will contain a label of its true classification. The role of the algorithm is to learn a general rule, expressed as a model, to map an input datum to a label. The algorithm will train itself against a subset of all data, and then will be evaluated against a test subset to detemine the accuracy of its labelling rule. The process of assigning labels to input data is known as classification.
\newline
\newline
The generation of this model is dependent on ``feature vectors'' which are extracted from the data as a normalisation step to facilitate statistical analysis. ``Features'' are properites of the data which help to develop rules for detecting patters, used in the eventual classification model. An example of a feature would be a the frequency of words associated with spam when attempting to filter emails.
\newline
\subsubsection{Feature Extraction}
The extraction of features is used as a normalisation step where possibly unstructured data is converted into a structured equivalant. For example, sentiment analysis of a work of text would involve generating a frequency histogram of work counts.
\newline
\newline
For image processing, feature extraction can involve resizing the image so that all data are of the same dimensions followed by converting a raster image to an equivalent matrix and possibly normalising the values of this matrix.
\newline
\subsubsection{MNIST Dataset}
The MNIST dataset is a collection of handwritten digits, known as the ``Hello World'' of machine learning. It consists of 60,000 training examples and 10,000 testing examples, which are all accurately labelled. This project will be using the MNIST dataset in the evaluation of a work stealing algorithm.
\newline
\subsection{Handwriting Recognition}
\subsection{Parameter Server Architecture}
This project will investigate the effects of work stealing in a distributed machine learning framework. The advantage of a distributed framework are decreased training time, redundancy in the event of a single machine failing, and allowing training to occur on datasets that cannot fit on a single machine.
\newline
\newline
In a single-worker parameter server environment there are two processes controlling the machine learning: the parameter server, acting as a datastore of the model, and the worker, updating the model based on new training data. Data is read by the single worker and processed in batches, referred to as ``minibatches'', and updates are sent to the parameter server. Processing is batched as the bandwidth between the worker and parameter server may be limited. The updates to the parameter server are also commutative and associative so a local reduction at the worker can occur before sending out the batched updates.
\newline
Figure \ref{ParameterServerArchitecture} contains a visualisation of the parameter server architecture.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{A demonstration of how multiple executions of the same task can interleave to return an incorrect result}
  \label{ParameterServerArchitecture}
\end{figure}

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{A list of all training images $N$ of size $n$}
 \Input{The size of each minibatch $m$}
 \BlankLine

 \For{$i \leftarrow 0$ \KwTo $N.length/m$}{
  \For{$j \leftarrow 0$ \KwTo $m$}{
    $error \longleftarrow 0$\;
    $n \longleftarrow N[j]$\;
    $n' \longleftarrow normaliseImageToMatrix(n)$\;
    $grad \longleftarrow extractGradients(n')$\;
    $prediction \longleftarrow model.train(grad)$\;
    $error \longleftarrow error + computeLoss(actual, prediction)$\;
   }
  $model.correctError(error)$\;
 }
 \caption{Single Worker Handwriting Image Recognition Algorithm}
 \label{SingleWorkerMachineLearningAlgorithm}
\end{algorithm}
\DecMargin{1em}
\medskip
In order to facilitate faster processing, multi-worker environments divide the set of minibatches between workers and workers are trained in parallel. There are several synchronisation strategies that can be applied for the collected updates at the end of every minibatch for each worker.



\subsubsection{Bulk Synchronous Parallel (BSP)}
This is the simplest form of multi-worker synchronisation. At the end of each minibatch the workers send the updates to the parameter server which collects them and will not allow workers to proceed to the next batch until all updates have been received. The advantage of this strategy is that it guarantees that all workers will converge on a single model because workers by definition cannot display more than a single round of divergence in their models. There are however significant overheads in terms of waiting for the slowest worker per round to synchronise with the master. Traffic patters are also quite bursty with the potential for network congestion at the beginning of each minibatch round. This model allows for the greatest progress in a single round because all workers have received the updates from all other workers before proceeding to the next round.

\subsubsection{Full Asynchronous}
If we consider $l$ to be the limit of the number of minibatch rounds between the slowest and fastest workers then we can consider BSP to have $l = 1$. In a Fully Asychronous model then $l = \infty$, i.e. workers proceed at their own rate unimpared by the progress of others. Fully asynchronous models will saturate compute capacity, a metric we wish to optimise for. However, because there is no guarantee on them receving updates from other workers before proceeding on to the next minibatch, there is the possibility that the worker's local model is stale and outdated. This reduces the efficacy of training and will result in a slowed convergence. Worker's local models may also diverge reuslting in an ineffective global model at the parameter server.

\subsubsection{Bounded Staleness}
In a Bounded Staleness model $l$ is configurable. In the event of a slow worker being caused by an transient delay, for example a pathological kernel scheduling, unaffected workers can continue to make progress and the delayed worker will hopefully catch up. If a worker is consistently slower, as could be the case with a less powerful host machine, then the same problems will occur with BSP, albeit with a difference of multiple minibatch rounds, rather than one.
\newline
\newline
In a conventional machine learning framework, the computation and updating of local parameters will be performed seperately. The intention is to saturate all possible resources, which is only attainable if the seperate processes are decoupled.
\newline
\newline
The algorithm for a mulit-worker environment is very similar to that of a single worker as can be seen in the difference between Algorithm \ref{SingleWorkerMachineLearningAlgorithm} and Algorithm \ref{MultiWorkerMachineLearningAlgorithm}. Note that the worker no longer accepts all training images, but rather a subset of them, and that the concept of a staleness bound has been introduced to prevent divergence between the workers.

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{A list of this worker's training images $N$}
 \Input{The size of each minibatch $m$}
 \Input{The limit of staleness between the slowest and fastest workers $l$}
 \BlankLine

 $round \longleftarrow 0$\;
 \For{$i \leftarrow 0$ \KwTo $N.length/m$}{
  \While{$round - l > globalRound$}{
    \tcc{Busy wait until the staleness limit has been reached}
  }
  \For{$j \leftarrow 0$ \KwTo $m$}{
    $error \longleftarrow 0$\;
    $n \longleftarrow N[j]$\;
    $n' \longleftarrow normaliseImageToMatrix(n)$\;
    $grad \longleftarrow extractGradients(n')$\;
    $prediction \longleftarrow model.train(grad)$\;
    $error \longleftarrow error + computeLoss(actual, prediction)$\;
   }
  \tcc{This will send updates to the parameter server}
  $model.correctError(error)$\;
  $round \longleftarrow round + 1$\;
 }
 \caption{Multiple Workers Handwriting Image Recognition Algorithm}
 \label{MultiWorkerMachineLearningAlgorithm}
\end{algorithm}
\DecMargin{1em}

\medskip
\medskip

Algorithm \ref{MultiWorkerMachineLearningAlgorithm} and Algorithm \ref{ModelUpdate} execute in parallel on seperate threads per worker to ensure the greatest use of resources possible.
\newline

\IncMargin{1em}
\begin{algorithm}[H]
  $globalRound \longleftarrow 0$\;
  \While{$true$}{
    $update \longleftarrow receiveParameterServerModelUpdate()$\;
    $model.applyUpdate(update)$\;
    $globalRound \longleftarrow globalRound + 1$\;
  }
 \caption{Model Update}
 \label{ModelUpdate}
\end{algorithm}
\DecMargin{1em}
\medskip

\newpage

\section{Work Stealing Algorithms}

To implement work stealing between workers in a parameter server architecture two algorithms were developed. One of which focused on a decentralised, worker-to-worker communication model, and another using the parameter server as a central co-ordinator. In this chapter I will discuss the implementation of both algorithms as well as considerations for real world use.

\subsection{Centralised Algorithm}

The centralised approach is based upon workers sending periodic updates to the master with their current status. The interval between these updates is configurable, but through evaluation of these intervals I have found that diminishing returns occur beyond TODO intervals. Figure \ref{IntervalGraph} details that beyond TODO intervals there exists a diminishing return in the benefit of finer progress granularity. 

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Graph showing update interval against relative performance.}
  \label{IntervalGraph}
\end{figure}

In the MapReduce paper \citep{dean2008mapreduce} the authors provide causes for stragglers such as "a machine with a bad disk may experience frequent correctable errors that slow its read performance from 30 MB/s to 1 MB/s" and "competition for CPU, memory, local disk, or network bandwidth". In a paper analysing network failures in data centres by Microsoft Research \citep{gill2011understanding} the main cause of failures was found to be software bugs in load balancers which caused many small packets such as ACKs and keep-alives to be dropped.
\newline
As mentioned previously, this project will be focussing on dealing with workers experiencing transient performance degradation. In the above examples, only the resource over-subscription and network packet drops will be considered. 

\newpage

\bibliographystyle{plain}
\bibliography{James-DavidCarr-MastersReport}

\end{document}










































