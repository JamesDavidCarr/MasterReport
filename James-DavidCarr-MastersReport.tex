\documentclass[12pt]{article}
\usepackage{url}
\usepackage{dirtytalk}
\usepackage[margin=1.1in]{geometry}
\usepackage[linesnumbered,resetcount,algosection]{algorithm2e}
\usepackage{listings}
\usepackage{color}
\usepackage[autostyle]{csquotes}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{amsthm}
\usepackage{chngcntr}

\newcommand\todo[1]{\textcolor{red}{#1}}

\urlstyle{same}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\lstset{language=Java}

\newtheorem{lemma}{Lemma}

\counterwithin{figure}{section}
\counterwithin{lemma}{section}
\AtBeginDocument{\counterwithin{lstlisting}{section}}

\begin{document}

\vspace*{\fill}
\centerline{\large{\textbf{Abstract}}}
Machine learning methods require many workers, all of whom iteratively analyse a dataset in parallel in a series of rounds, collecting their progress centrally at the end of each round. By implementing a work stealing algorithm, the delay introduced by the slowest workers each round will be mitigated. The goal of this project is to implement a work stealing algorithm and validate that it does provide a throughput increase of rounds completed per second. The algorithm is validated on the MNIST trainging set and is built on top of the work of the LSDS group at Imperial College London. An analysis and evaluation will be discussed in this report.
\vspace*{\fill}

\newpage

\tableofcontents

\newpage

\section{Chapter 1 - Introduction}

\subsection{Motivation}

The volume of data produced per year is increasing exponentially through social media, embedded Internet of Things sensors, and even financial records. Analysis of this data is incredibly valuable to the data owners: email services can better detect spam, banks can monitor for fraud, and cities can be managed more efficiently.
\newline
\newline
In a report by The Department for Business Innovation and Skills\cite{smartCities}, it was estimated that the \say{global market for smart urban systems for transport, energy,
healthcare, water and waste will amount to around \$400 Billion pa. by 2020}. Through more intelligent city management and improved data collection \say{we can manage cities more effectively} and \say{anticipate and solve problems more cost effectively}.
\newline
\newline
In order to achieve these goals of cost effective and efficient public services the data collected must be analysed. Through machine learning techniques, automated systems can ensure continuous monitoring of services such as traffic and crime, and anticipate future events based on past data.
\newline
\newline
Machine learning on large data sets has been common in the private sector for some while now. Google use emails to detect and prevent spam, Amazon recommend products to purchase based on a customer's history and other customers making similar purchases, and Facebook use facial recognition software to automatically ``tag'' friends in photos.
\newline
\newline
At a high level of abstraction, these machine learning models work by ingesting large volumes of data, detecting patterns, and then mapping further inputs to outputs. For example, when a picture containing a face is uploaded to Facebook they will attempt to map it to users they believe to be in the picture. The initial process of ingesting the data however can be incredibly time consuming, Facebook reported in 2010\cite{facebookHaystack} that they stored 260 billion images representing over 20PB of data.
\newline
\newline
In order to speed the process of reading and analysing data of this size, the process is sharded between many workers. Each worker analyses a subset of the data and aggregating their results in a central model in a process called training. As additional data is received, there will be an attempt to classify them against the central model which will assign a label identifying the data. The sharding of data presents new challenges in an attempt to reduce training times and simultaneously improving the accuracy of data labelling.
\newline
\newline
One of the major problems with any sharded system is the ability to handle slow workers, also known as stragglers. Many algorithms in the field of machine learning are synchronous and require the termination of all workers before proceeding to the next round of computation and are therefore limited by the slowest worker. If the effects of these stragglers were to be alleviated, it would improve the performance of the algorithm and thus complete the training phase sooner.

\newpage

\subsection{Problem Statement}
There are currently many Machine Learning frameworks used in industry, and as with most industrial software, performance is a major concern. These frameworks are typically able to run in a distributed fashion across many workers in order to improve performance. In any distributed environment, there will be a distribution of execution run times, caused by non-deterministic factors such as kernel scheduling, network packet drops, and cache misses. In some pathological cases there will be workers who are significantly delayed compared to other worker. These workers are referred to as stragglers. Certain machine learning algorithms are dependent on the slowest workers and will block until completion of the slowest straggler.
\newline
Workers can be delayed for both transient reasons such as temporary network degradation or resource over subscription, but also for permanent reasons such as disk drive failures. This project will focus on mitigating the effects of transient failures. If a worker experiences a permanent failure, the larger task can either be aborted, or work can be redistributed amongst other, healthy workers. In an initial revision of Google's MapReduce \citep{dean2008mapreduce}, the authors write that \say{given that there is only a single master, its failure is unlikely; therefore our current implementation aborts the MapReduce computation if the master fails}. A solution used by many distributed frameworks such as MapReduce and Apache Spark \cite{zaharia2012resilient} is to \say{re-run it on another node}.
\newline
These solutions to permanent worker failures have been very successful and as such the problem of failing workers is solved in industry. This project will focus on temporary performance degradation of workers, especially so in an environment of limited physical resources. Forward progress will be made using the workers, however they will operate at their maximum capacity and will not impair other workers.
\newline
The aim of this project is to develop and evaluate algorithms to mitigate the effects of these slowest stragglers through the use of work-stealing. We also aim to develop such an algorithm that does not negatively affect the accuracy of the system, as this is the primary objective, with performance being secondary.

\newpage

\subsection{Objective}
The objective of this project are:

\begin{enumerate}
\item Evaluate the performance improvements through the use of dynamically scheduled threads rather than statically scheduled.
\item Develop a centralised and decentralised algorithm so that faster workers may steal work from slower workers.
\item Demonstrate the effect of these two algorithms in both the presence and absence of stragglers.
\end{enumerate}

\newpage

\section{Chapter 2 - Background}

\subsection{Stragglers}

In the MapReduce paper \citep{dean2008mapreduce} the authors provide causes for stragglers such as \say{competition for CPU, memory, local disk, or network bandwidth}. Google's cluster management system, Borg \cite{43438} is responsible for task allocations where a pool of resources are used to distribute tasks over. In the Borg paper the authors detail how initial versions of Borg did not have sufficient sub-resource isolation. Tasks would be assigned CPU limits in response to utilisation rather than applying a limit beyond which a task could not exceed. Subsequent solutions used Linux c-group resource containers to apply limits to rate-based resources such as CPU usage, network I/O, and disk I/O. These controls do not guarantee unimpaired performance though as the paper highlights that \say{L3 cache pollution still happens}, which would cause a degradation to performance of both tasks as they cause thrashing in the processor cache.
\newline
If the machine learning workers were co-located alongside other short-duration CPU intensive tasks, even with the appropriate kernel-level tooling to prevent performance degradation, it is likely that performance would be affected, and the worker would become a straggler. Co-location is often used to increase the utilisation of all resources, and a solution to the problem of stragglers which accounts for co-location is more applicable in real-world uses. TODO these previous two paragraphs should be somewhere else.

\subsection{Work Stealing}

In a system comprised of many independent workers each of which have a set of tasks which they are reponsible for computing, there will be some variance in the rate of task completions among workers.
\newline
\newline
The principle behind work stealing is to increase the total utilisation of resources in a processing system by exchanging tasks between workers according to their usage. Each worker maintains a queue of tasks to run, and when a worker has completed its tasks it will ``steal'' tasks from another worker's queue.
\newline
\newline
The Java Runtime Environment contains the ForkJoinPool\cite{javaThreads}, a convenient concurrency primitive where tasks can be submitted to a pool of workers and executed in parallel. Each worker maintains two separate task queues, one for work it has been allocated, and another for work that it has stolen. Tasks that are submitted to the worker pool can optionally be written so that they are able to be decomposed into smaller tasks, which can be divided among workers.
\newline

\IncMargin{1em}
\begin{algorithm}[H]
  \eIf{my portion of the work is small enough}{
   do the work directly\;
   }{
   split my work into two pieces\;
   invoke the two pieces and wait for the results\;
  }
 \caption{ForkJoin Algorithm}
 \label{ForkJoinAlgorithm}
\end{algorithm}
\DecMargin{1em}
\medskip
Algorithm \ref{ForkJoinAlgorithm} contains the pseudocode used in the Java language documentation to explain the ForkJoinPool framework. The ``fork'' occurs on line 4 where a task is divided into two subtasks, and the ``join'' is line 5 when waiting for the results.

\newpage

\begin{lstlisting}[caption={An implementation of the Fibonacci algorithm using the ForkJoinFramework taken from the Java documentation},label=JavaRecursive]
public class Fibonacci extends RecursiveTask<Integer> {
  final int n;
  Fibonacci(int n) { this.n = n; }
  Integer compute() {
    if (n <= 1)
      return n;
    Fibonacci f1 = new Fibonacci(n - 1);
    f1.fork();
    Fibonacci f2 = new Fibonacci(n - 2);
    return f2.compute() + f1.join();
  }
}
\end{lstlisting}



It is important to note that work stealing occurs at both a user-level and kernel-level on a modern operating system. However, each of these has a different use case for work stealing. In the above Java example, a user-level program, the use of a ForkJoinPool was to convert a serial program into a parallel equivalent with a decreased runtime. Conversely, the Linux scheduler, a kernel-level program, uses work stealing to ensure fair access to resources for users. The programs presented in this paper are user-level programs and as such they are focused on improving performance.

\subsubsection{Dynamic vs Static Scheduling}
In the area of parallel computing, there exists the concept of an "embarrassingly parallel" task, which involves a set of operations with no inter-operation dependencies. A scalar-vector product is such a task because each element of the vector can be multiplied independently. Embarrassingly parallel tasks are limited by the computational resources available until an upper bound is met. The upper bound for a scalar-vector product is the number of elements in the vector. Beyond this amount, no additional resources will be useful as there is no work available for them to execute.
\newline
We shall now consider the case in which there is a pool of work to complete, and fewer computational resources than the amount of work, implying that each resource will be required to execute at least one piece of work. In such a model we can assign work to these resources with either a dynamic or static schedule.
\newline
A static method is one where resources are assigned work ahead of time, usually using some combination of resource identifiers and work identifiers. An example of a statically scheduled algorithm of a scalar-vector product is presented in Algorithm \ref{StaticSchedule}.

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{A reference $N$ to a vector of size $n$}
 \Input{The scalar $s$}
 \Input{This resource's identifier $i$}
 \Input{The number of resources $r$}
 \BlankLine

 $j = i$\;
 \While{$j < n$}{
   $N[j] \longleftarrow N[j]*s$\;
   $j \longleftarrow j + r$\;
 }
 \caption{Statically Scheduled Scalar-Vector Product}
 \label{StaticSchedule}
\end{algorithm}
\DecMargin{1em}
\medskip

The above algorithm is conceptually very simple, and is guaranteed to eventually multiply every element in the vector with the given scalar in the absence of resource failures.
\newline
However, under this schedule it is possible for the performance of the overall algorithm to be impacted by a single, slower resource. Although all resources operate independently the overall completion of the algorithm is dependent on the slowest result being completed.
\newline
Some optimisations may include grouping together the elements that a resource will need to access to improve cache performance, but these are inconsequential to the problem of dealing with the slowest resource.
\newline
However, what if instead of resources being assigned work ahead of time, they were able to complete work at the fastest rate of which they are capable. This would alleviate the impact caused by the slowest worker simply due to it processing fewer tasks.
\newline
\newline
Dynamic scheduling usually requires the use of some piece of state to track progress, and resources modify the state to represent work completion. These pieces of state are usually atomic or thread-safe data structures in order to avoid the risk of race conditions.
\newline
Continuing the above example of a scalar-vector product Algorithm \ref{DynamicSchedule} presents an alternative to Algorithm \ref{StaticSchedule} which is more resilient against slow resources.
\newline

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{The vector $N$ of size $n$}
 \Input{The scalar $s$}
 \Input{The number of resources $r$}
 \Input{An integer $c$ to represent the current element being processed, and that can be incremented atomically}
 \BlankLine

 $j = 0$\;
 \While{$(j \leftarrow c.getAndInc()) < n$}{
   $N[j] \longleftarrow N[j]*s$\;
 }
 \caption{Dynamically Scheduled Scalar-Vector Product}
 \label{DynamicSchedule}
\end{algorithm}
\DecMargin{1em}
\medskip

In Algorithm \ref{DynamicSchedule} the variable $j$ is a thread-local variable used to store the return value of the $getAndInc$ operation on the atomic integer which atomically returns the current value and incrementing it. Because the value of $c$ only monotonically and atomically increases, it is not possible for multiple threads to have the same value of $c$, and therefore each element of $N$ will only be modified once. It can be seen that the rate of completion of such an algorithm is now less vulnerable to the slowest resource as faster resources can effectively "steal" some of the work the slower resource would have been allocated under a static schedule.
\newline
If the relative performances of the resources were known ahead of time, it would be possible to construct a static scheduling which would take account of this and attempt to weight the amount of work a resource receives by its performance. However, such an algorithm would be unable to deal with dynamic variations in performance often encountered in real world use cases. There are many non-deterministic factors affecting the performance of the overall algorithm and a static scheduler is not flexible enough to adapt to consider these factors when assigning work.
\newline
It is also important to realise that not all algorithms could benefit from a dynamic scheduling. If the workers are co-located on a single, physical machine then the average case for inter-resource communication will be substantially lower than for inter-machine communication. In the above example, the atomically incrementable variable could be accessed over a network connection, but the overhead of such an exchange would likely outweigh any performance benefits of work-stealing for most application executions.
\newline
There are pathological cases in which different schedules provide no benefit. Consider a resource that has crashed in an environment with no upper bound on execution time of a task. Other workers will be unable to determine if a worker has either crashed or is currently processing.
\newline
Using the definition of a synchronous algorithm from \cite{mageeanalyzing}.

\blockquote{Synchronous distributed systems are those in which there is assumed
to be a known upper bound on each processing step, a known upper bound on
message transmission and processes have perfectly synchronized physical
clocks.}

From this definition it can be seen that both Algorithm \ref{StaticSchedule} and Algorithm \ref{DynamicSchedule} are not synchronous as there is no bound on processing steps, and it cannot be assumed that all workers have synchronised physical clocks, therefore stragglers appear. The problem is handling the case where a single resource has either crashed, or could be taking an indefinite amount of time to process. In the case where execution is blocked on a single node for an arbitrary amount of time, no scheduling system will be of use.

\subsubsection{Side Effect Free Operations and Idempotence}
Algorithms \ref{StaticSchedule} and \ref{DynamicSchedule} are both stateful solutions to the problem of a scalar vector product. This is because they are assumed to be run on multiple resources who will modify some global piece of state, in this case the vector.
\newline
We will instead consider an alternative solution which decouples the computation from the state modification. A set of $r$ resources shall be divided into a single, central resource that will manage updates to the input vector, and $r-1$ resources responsible for computing the product of a scalar and vector element.

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{The vector $N$ of size $n$}
 \Input{The scalar $s$}
 \Input{The number of resources $r$, $r < n$}
 \Output{The vector $N$ after a scalar-vector product with $s$}
 \BlankLine

 $j = 0$\;
 \For{$i \gets 1$ \KwTo $r$}{
   $sendTaskToResource(i, multiply, N[j], s)$\;
   $j \longleftarrow j + 1$\;
 }
 \While{$true$}{
   $index, result, resourceID = fetchLatestResult()$\;
   $N[index] \longleftarrow result$\;
   \If{$j = n - 1$}{
     $break$\;
   }
   $sendTaskToResource(resourceID, multiply, N[j], s)$\;
   $j \longleftarrow j + 1$\;
 }
 \caption{Central resource responsible for state manipulation}
 \label{StatelessProduct}
\end{algorithm}
\DecMargin{1em}
\medskip

The function $sendTaskToResource$ used in Algorithm \ref{StatelessProduct} instructs the resource with the identifier equal to the first parameter to run the task identified by the second parameter using all subsequent parameters as task arguments.
\newline
In this example the other resources will be executing a simple multiplication of their inputs: the vector element and the scalar. Note that the actual computation here is very simply purely for demonstrative purposes. In a real-world application, the work would be much more intensive and justify the need for multiple workers. This multiplication is also a function as it is guaranteed that given the same inputs that the same output will be returned.
\newline
The benefit of this system is that worker computations are now side-effect free as they only operate on state passed in as a parameter, rather than any global state. If the pathological case previously discussed where a single worker spends a potentially unbounded amount of time were to occur it can now be mitigated. The solution would be to wait until all work has been assigned and then for all outstanding tasks issue a second request to a second resource and the central resource will use whichever value returns soonest. This is only possible because the operations executed on each resource are side-effect free functions which guarantee that they will provide the same output give identical inputs.
\newline
Such a solution is not viable without creating side effect free tasks as if two executions of the task were to occur they could both modify the external state.
\newline
Consider Figure \ref{OverlappingExecution} which visualises how the overlapping of two executions could result in an incorrect result. There are solutions which account for this, however, it is much simpler to develop a solution for which this problem cannot occur.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{A demonstration of how multiple executions of the same task can interleave to return an incorrect result}
  \label{OverlappingExecution}
\end{figure}

In Algorithm \ref{StatelessProduct} the only state modification is assignment which is an idempotent operation meaning that it can occur multiple times and still return an identical result. Therefore if two resources were to return their identical results before the termination of the algorithm it would not cause the result to be incorrect.
\newline
This approach has been used in industry with a notable example being Google's MapReduce framework. The white paper released detailing the framework \cite{dean2008mapreduce} contained the following explanation of the framework's solution to dealing with stragglers.

\blockquote{When a MapReduce operation is close
to completion, the master schedules backup executions
of the remaining in-progress tasks. The task is marked
as completed whenever either the primary or the backup
execution completes.}

\newpage

\subsection{Machine Learning}
Machine Learning is a field of Computer Science associated with developing algorithms that can learn from and make predictions from data\cite{machineLearningDef}. It is closely related to the fields of Computational Statistics and Mathematical Optimisations, and results in computers which are able to detect patterns in data without explicit programming. The use of machine learning has spread well into industry and is now commonly used in areas such as finance, medicine, and telecommunications.
\newline
\subsubsection{Supervised Learning}
This work will be focused on an area of machine learning called \textbf{Supervised Learning}. In supervised learning each input datum to the algorithm will contain a label of its true classification. The role of the algorithm is to learn a general rule, expressed as a model, to map an input datum to a label. The algorithm will train itself against a subset of all data, and then will be evaluated against a test subset to detemine the accuracy of its labelling rule. The process of assigning labels to input data is known as classification.
\newline
\newline
The generation of this model is dependent on ``feature vectors'' which are extracted from the data as a normalisation step to facilitate statistical analysis. ``Features'' are properites of the data which help to develop rules for detecting patters, used in the eventual classification model. An example of a feature would be a the frequency of words associated with spam when attempting to filter emails.
\newline
\subsubsection{Feature Extraction}
The extraction of features is used as a normalisation step where possibly unstructured data is converted into a structured equivalant. For example, sentiment analysis of a work of text would involve generating a frequency histogram of work counts.
\newline
\newline
For image processing, feature extraction can involve resizing the image so that all data are of the same dimensions followed by converting a raster image to an equivalent matrix and possibly normalising the values of this matrix.
\newline
\subsubsection{MNIST Dataset}
The MNIST dataset is a collection of handwritten digits, known as the ``Hello World'' of machine learning. It consists of 60,000 training examples and 10,000 testing examples, which are all accurately labelled. This project will be using the MNIST dataset in the evaluation of a work stealing algorithm.
\newline
\subsection{Handwriting Recognition}
\subsection{Parameter Server Architecture}
This project will investigate the effects of work stealing in a distributed machine learning framework. The advantage of a distributed framework are decreased training time, redundancy in the event of a single machine failing, and allowing training to occur on datasets that cannot fit on a single machine.
\newline
\newline
In a single-worker parameter server environment there are two processes controlling the machine learning: the parameter server, acting as a datastore of the model, and the worker, updating the model based on new training data. Data is read by the single worker and processed in batches, referred to as ``minibatches'', and updates are sent to the parameter server. Processing is batched as the bandwidth between the worker and parameter server may be limited. The updates to the parameter server are also commutative and associative so a local reduction at the worker can occur before sending out the batched updates.
\newline
Figure \ref{ParameterServerArchitecture} contains a visualisation of the parameter server architecture.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{A demonstration of how multiple executions of the same task can interleave to return an incorrect result}
  \label{ParameterServerArchitecture}
\end{figure}

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{A list of all training images $N$ of size $n$}
 \Input{The size of each minibatch $m$}
 \BlankLine

 \For{$i \leftarrow 0$ \KwTo $N.length/m$}{
  \For{$j \leftarrow 0$ \KwTo $m$}{
    $error \longleftarrow 0$\;
    $n \longleftarrow N[j]$\;
    $n' \longleftarrow normaliseImageToMatrix(n)$\;
    $grad \longleftarrow extractGradients(n')$\;
    $prediction \longleftarrow model.train(grad)$\;
    $error \longleftarrow error + computeLoss(actual, prediction)$\;
   }
  $model.correctError(error)$\;
 }
 \caption{Single Worker Handwriting Image Recognition Algorithm}
 \label{SingleWorkerMachineLearningAlgorithm}
\end{algorithm}
\DecMargin{1em}
\medskip
In order to facilitate faster processing, multi-worker environments divide the set of minibatches between workers and workers are trained in parallel. There are several synchronisation strategies that can be applied for the collected updates at the end of every minibatch for each worker.



\subsubsection{Bulk Synchronous Parallel (BSP)}
This is the simplest form of multi-worker synchronisation. At the end of each minibatch the workers send the updates to the parameter server which collects them and will not allow workers to proceed to the next batch until all updates have been received. The advantage of this strategy is that it guarantees that all workers will converge on a single model because workers by definition cannot display more than a single round of divergence in their models. There are however significant overheads in terms of waiting for the slowest worker per round to synchronise with the master. Traffic patters are also quite bursty with the potential for network congestion at the beginning of each minibatch round. This model allows for the greatest progress in a single round because all workers have received the updates from all other workers before proceeding to the next round.

\subsubsection{Full Asynchronous}
If we consider $l$ to be the limit of the number of minibatch rounds between the slowest and fastest workers then we can consider BSP to have $l = 1$. In a Fully Asychronous model then $l = \infty$, i.e. workers proceed at their own rate unimpared by the progress of others. Fully asynchronous models will saturate compute capacity, a metric we wish to optimise for. However, because there is no guarantee on them receving updates from other workers before proceeding on to the next minibatch, there is the possibility that the worker's local model is stale and outdated. This reduces the efficacy of training and will result in a slowed convergence. Worker's local models may also diverge reuslting in an ineffective global model at the parameter server.

\subsubsection{Bounded Staleness}
In a Bounded Staleness model $l$ is configurable. In the event of a slow worker being caused by an transient delay, for example a pathological kernel scheduling, unaffected workers can continue to make progress and the delayed worker will hopefully catch up. If a worker is consistently slower, as could be the case with a less powerful host machine, then the same problems will occur with BSP, albeit with a difference of multiple minibatch rounds, rather than one.
\newline
\newline
In a conventional machine learning framework, the computation and updating of local parameters will be performed seperately. The intention is to saturate all possible resources, which is only attainable if the seperate processes are decoupled.
\newline
\newline
The algorithm for a mulit-worker environment is very similar to that of a single worker as can be seen in the difference between Algorithm \ref{SingleWorkerMachineLearningAlgorithm} and Algorithm \ref{MultiWorkerMachineLearningAlgorithm}. Note that the worker no longer accepts all training images, but rather a subset of them, and that the concept of a staleness bound has been introduced to prevent divergence between the workers.

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{A list of this worker's training images $N$}
 \Input{The size of each minibatch $m$}
 \Input{The limit of staleness between the slowest and fastest workers $l$}
 \BlankLine

 $round \longleftarrow 0$\;
 \For{$i \leftarrow 0$ \KwTo $N.length/m$}{
  \While{$round - l > globalRound$}{
    \tcc{Busy wait until the staleness limit has been reached}
  }
  \For{$j \leftarrow 0$ \KwTo $m$}{
    $error \longleftarrow 0$\;
    $n \longleftarrow N[j]$\;
    $n' \longleftarrow normaliseImageToMatrix(n)$\;
    $grad \longleftarrow extractGradients(n')$\;
    $prediction \longleftarrow model.train(grad)$\;
    $error \longleftarrow error + computeLoss(actual, prediction)$\;
   }
  \tcc{This will send updates to the parameter server}
  $model.correctError(error)$\;
  $round \longleftarrow round + 1$\;
 }
 \caption{Multiple Workers Handwriting Image Recognition Algorithm}
 \label{MultiWorkerMachineLearningAlgorithm}
\end{algorithm}
\DecMargin{1em}

\medskip
\medskip

Algorithm \ref{MultiWorkerMachineLearningAlgorithm} and Algorithm \ref{ModelUpdate} execute in parallel on seperate threads per worker to ensure the greatest use of resources possible.
\newline

\IncMargin{1em}
\begin{algorithm}[H]
  $globalRound \longleftarrow 0$\;
  \While{$true$}{
    $update \longleftarrow receiveParameterServerModelUpdate()$\;
    $model.applyUpdate(update)$\;
    $globalRound \longleftarrow globalRound + 1$\;
  }
 \caption{Model Update}
 \label{ModelUpdate}
\end{algorithm}
\DecMargin{1em}
\medskip

\newpage

\section{Work Stealing Algorithms}

To implement work stealing between workers in a parameter server architecture two algorithms were developed. One of which focused on a decentralised, worker-to-worker communication model, and another using the parameter server as a central co-ordinator. This chapter will discuss the implementation of both algorithms as well as considerations for real world use.

\subsection{Centralised Model}

The centralised approach is based upon workers sending periodic updates to the master with their current status. The interval between these updates is configurable, but through evaluation of these intervals I have found that diminishing returns occur beyond TODO intervals as can be seen in Figure \ref{IntervalGraph} which shows that there exists a diminishing return in the benefit of finer progress granularity.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Graph showing update interval against relative performance.}
  \label{IntervalGraph}
\end{figure}

Updates to the master occur multiple times per round for each worker, and each round is usually completed within 40-80ms per worker depending on number of cores available. The diminishing returns occur due to a linear performance profile during each round for workers. In the event of a performance degradation, the cause is unlikely to be resolved within a single round. Therefore while certain rounds contain the start of a transient event that affects performance, most rounds occur in either an optimal or degraded state, and do not alter between within a single round. This can be seen on Figure \ref{RoundDegradation} which shows how transient events affect multiple rounds.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Graph showing update interval against relative performance.}
  \label{RoundDegradation}
\end{figure}

Updates beyond a certain frequency simply represent no new information and simply cause to increase load on the parameter server and utilise excessive bandwidth.
\newline
\subsubsection{Update Based Approach}
Initially, four centralised notification exchange schemes were considered and benchmarked to determine which provided the greatest benefit against stragglers.

\begin{enumerate}
\item No updates and message sent before end of round.
\item No updates and message sent at end of round.
\item Updates and message sent before end of round.
\item Updates and message sent at end of round.
\end{enumerate}

The distinction between \say{end of round} and \say{before end of round} refer to when the centralised master would send the messages indicating to workers how they would be affected by work stealing. Immediately the first approach can be ruled out as the master cannot know whether a worker is close to completing their round without updates. Such a system could have depended upon prior rounds and projecting round completion into the future, but this would be vulnerable to unexpected delays, which was the intention of the project.
\newline
The benefits of sending messages before the end of the round is that it allows the worker that is stealing to pre-load their image buffer so that immediately upon terminating its initial batch it could process the stolen work. There is a concern with this approach that communication delays would cause the message alerting the victim of work-stealing to not execute certain tasks to arrive after they had been processed. TODO explain why this is not ideal, but won't significantly affect performance.
\newline
Figure \ref{RepeatedEvents} shows the number of repeated images processed across TODO rounds of training. This is a measure that should be minimised and it can be seen that \say{Updates and messages sent before end of round} achieves the lowest score in this metric. Figure \ref{EventProcessingPerformance} shows the relative performance of the various schemes in the presence of stragglers, and again say{Updates and messages sent before end of round} is shown to be the superior choice.
\newline

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Graph showing notification schemes against number of repeated images. Lower is better.}
  \label{RepeatedEvents}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Graph showing notification schemes against number of repeated images. Lower is better.}
  \label{EventProcessingPerformance}
\end{figure}

\subsubsection{Centralised Algorithm for Workers}

Algorithm \ref{CentralisedAlgorithmWorker} details the algorithm that the workers execute. The underlying framework in this project provides the mechanisms for message delivery between workers. Messages contain a string identifying the type of message, for example either an \say{update} message to the master, or a \say{begin} message to the workers.
\newline
In Algorithms \ref{CentralisedAlgorithmWorker} and \ref{CentralisedAlgorithmServer} this messaging framework will be represented using a function \say{on}. The \say{on} function takes as parameters: the message string identifier and optionally additional parameters required to process the message contents.
\newline

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwProg{Fn}{Function}{}{}
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{The size of each minibatch $n$}
 \Input{The interval to send messages $interval$}
 \BlankLine
 \Fn{on("beginRound", n, interval)}{
   $stolenThisRound \longleftarrow false$\;
   $N \longleftarrow nextMiniBatch()$\;
   \For{$i \gets 1$ \KwTo $n$}{
      \If{i mod interval = 0}{
        $sendUpdateToMaster()$\;
      }
      $train(N[i])$\;
   }
   $awaitResponse()$\;
   \If{not stolenThisRound}{
     $sendResultToServer()$\;
   }
 }
 \BlankLine
 \BlankLine
 \setcounter{AlgoLine}{0}
 \BlankLine
 \Fn{on("nack")}{
 }
 \BlankLine
 \BlankLine
 \setcounter{AlgoLine}{0}
 \Input{The amount of work stolen $count$}
 \BlankLine
 \Fn{on("lose", count)}{
   $skipImages(count)$\;
 }
 \BlankLine
 \BlankLine
 \setcounter{AlgoLine}{0}
 \Input{The amount of work to steal $count$}
 \Input{The offset at which to read stolen work from $offset$}
 \BlankLine
 \Fn{on("steal", count, offset)}{
   $stolenThisRound \longleftarrow true$\;
   $N \longleftarrow readImages(count, offset)$\;
   \For{$i \gets 1$ \KwTo $count$}{
     $train(N[i])$\;
   }
   $sendResultToServer()$\;
 }
 \caption{Centralised Algorithm executed by workers}
 \label{CentralisedAlgorithmWorker}
\end{algorithm}
\DecMargin{1em}
\medskip

In Algorithm \ref{CentralisedAlgorithmWorker}, $sendUpdateToMaster()$ and $sendResultToServer()$ are simply convenience methods to communicate with the parameter server, and will not be included for clarity. The $train$ function trains the machine learning model with the specified image. $skipImages$ will skip the specified images from the end of the worker's current minibatch. A \say{steal} message will instruct the worker to read the images that another worker has been too slow to process. \say{lose} is the equivalent call to the victim of work stealing to indicate that a number of images have been lost and that they should skip them when training. \say{nack} is used to indicate that no action should be taken this round as all workers are operating at a rate similar enough that any attempt at work stealing would incur additional overhead.

\subsubsection{Causality}
It has been observed that there exists a skew amongst a set of physical clocks, that is, they were not all perfectly synchronised. The lack of synchronisation results in the inability to use physical timestamps to represent causality between events as visualised in Figure \ref{PhysicalTimestamps}.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Visualisation of how physical timestamps cannot be used to represent causality.}
  \label{PhysicalTimestamps}
\end{figure}

In his 1978 work \say{Time, Clocks, and the Ordering of Events in a Distributed System} \cite{lamport1978time} Leslie Lamport proposed the idea of logical clocks. These would be used to represent causality because they were dependent upon changes in the state of a system, rather than the progression of time. Upon the change of state in a system the clocks would increment, and the relationship between two events as shown in Lemma \ref{LogicalOrderingLemma}.

\begin{lemma}
Given two events $a$ and $b$ where event $a$ occurred before event $b$ then the following relation holds.

$a \rightarrow b \longrightarrow C(a) < C(b)$
\label{LogicalOrderingLemma}
\end{lemma}
$C(a)$ is the value of the Lamport logical clock at the time that event $a$ occurred.
\newline

 Figure \ref{LogicalTimestamps} demonstrates this property and how an equivalent physical clock based system would have failed.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Visual comparison of logical and physical clocks.}
  \label{LogicalTimestamps}
\end{figure}

In Algorithm \ref{CentralisedAlgorithmWorker} there is a call to $awaitResponse()$. This is a method which blocks execution until the worker has received either a \say{nack}, \say{lose}, or \say{steal} message and handled its body. Message delivery in this system is designed to be asynchronous, so that messages can be received and handled simultaneously. The system has also been designed so that there is a guarantee that a worker cannot receive a \say{beginRound} message until the completion of this current round. The usage of this message can be seen as an equivalent to a logical clock in that it imposes an ordering on the modification of state in the system. If a worker were to steal work it can be ensured that they would not call $sendResultToServer$ more than once per round. Figure \ref{awaitResponse} shows the necessity of such a blocking call and how the logical ordering of events is able to prevent race conditions in a way that physical clocks can not.
\newline
There exists an implicit Lamport logical clock which imposes a logical ordering between the sending of updates to the master and whether or not to immediately send a response to the master after the completion of training. It ensures that the $sendResultToServer$ event must occur after the instruction issued by the server by implicitly incrementing a Lamport logical clock only after a message from the server has been processed.
\newline
As mentioned previously, there is the potential for some images to be repeated while processing. If the server instructs a worker to skip images and the worker receives the message after beginning to process images then the will be processed twice. A variant of this algorithm which utilises a Two-Phase Commit would alleviate this problem. The server would send an initial \say{commit-request} message to workers and would receive responses indicating whether they were prepared for work stealing. The preparation would be whether the victim had not already processed the work to be stolen. Upon a successful round of responses the master would then send a \say{commit} message which would instruct the work stealing to occur. If the initial responses indicated that work stealing would not be possible this round then the master would send an \say{abort} message and no work stealing would occur.
\newline
This algorithm would achieve resiliency against the same image being processed multiple times, however the Two-Phase Commit is known for being a high overhead approach, and in this scenario the overhead outweighs the benefits.
\newline
In the current implementation of Algorithm \ref{CentralisedAlgorithmWorker} the server will send a response to workers who will steal work before they have finished their current batch. This is to achieve greater performance by pipelining the tasks that the worker will perform. While the current batch of images is being processed, new images are being read. Figure \ref{RepeatedEvents} shows that this approach has led to TODO repeated events, and has not impacted accuracy.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Explanation of how physical timestamps cannot be used to represent causality.}
  \label{awaitResponse}
\end{figure}

\subsubsection{Centralised Algorithm for Parameter Server}

The algorithm which is executed by the parameter server is primarily centred around worker co-ordination and updating the central model, and is represented by Algorithm \ref{CentralisedAlgorithmServer}.

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwProg{Fn}{Function}{}{}
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{The result of a worker's training $result$}
 \BlankLine
 \Fn{on("receiveResult", result)}{
   $globalModel.add(result)$\;
   $responses \longleftarrow responses + 1$\;
   \If{$responses = numberOfWorkers$}{
     $sendToWorkers("beginRound")$\;
     $responses \longleftarrow 0$\;
     $stolenThisRound \longleftarrow false$\;
   }
 }
 \BlankLine
 \BlankLine
 \setcounter{AlgoLine}{0}
 \BlankLine
 \Input{The worker sending the message $worker$}
 \Input{The latest value trained $value$}
 \BlankLine
 \Fn{on("update", worker, index)}{
   $workerProgress[worker] = index$\;
   \If{$index > stealingThreshold \land \neg stolenThisRound$}{
     $slowestWorker \longleftarrow determineSlowestWorker(workerProgress)$\;
     $workToSteal \longleftarrow calculateWorkToSteal(slowestWorker)$\;
     \uIf{$workToSteal \geq minimumStealingAmount$}{
        $send(slowestWorker, "lose", workToSteal)$\;
        $send(worker, "steal", workToSteal)$\;
        $sendTo(allWorkers - slowestWorker - worker, "nack")$\;
        $stolenThisRound \longleftarrow true$\;
     }
     \Else{
        $sendToAllWorkers("nack")$\;
     }
   }
 }
 \BlankLine
 \caption{Centralised Algorithm executed by parameter server}
 \label{CentralisedAlgorithmServer}
\end{algorithm}
\DecMargin{1em}
\medskip

Algorithm contains some convenience methods for illustrative purposes, whose contents will not be shown for brevity. The parameter server contains a variable, $responses$ which tracks the number of worker models that the server has received in a single round. $workerProgress$ is a map with keys corresponding to the identifiers of workers, and values of the latest result received from each worker.
\newline
The $send$ function is used to deliver messages to the specified worker, using the second parameter as the message identifier and all remaining parameters as message contents. $sendTo$ broadcasts a message to a specified subset of all workers, and $sendToAllWorkers$ is a simple broadcast.
\newline
$determineSlowestWorker$ traverses the mapping pairs to determine which of the workers has the lowest associated value, indicating that the worker has made the least progress this round. $calculateWorkToSteal$
is a function which will determine the amount of work to steal from the slowest worker, and this amount will then be evaluated to determine whether it will be time effective to steal work. The benefit of work stealing is that the overhead of exchanging work is outweighed by the time saved through slower workers having less work to process. If the amount of work that could be stolen this round is too low then it will be more effective to let all workers finish without the aide of work stealing.

\subsubsection{Minimum Sufficient Work Stealing Value}
The limit on the minimum amount of work that can be stolen at a benefit to the system is determined through the evaluation of the costs associated with work stealing. Initially a model will be built assuming that updates are not sent, and a simpler model will be developed once updates are added to the system.
\newline
Several per-system and per-round values are used in the generation of the model.
$t_1$ represents the time of completion of the fastest worker.
$t_2$ represents the time of completion of the slowest worker.
$r_1$ represents the rate of training for the fastest worker.
$r_2$ represents the rate of training for the slowest worker.
$l$ is the worker-parameter server message delay time.
$i$ is the time taken for the fastest worker to read the images stolen from the slowest worker.
$s$ is the time taken to send the round model from a worker to the parameter server.
$x$ is the number of images that have been stolen.

We can formulate these terms into the following approximation modelling the effects of work stealing.
\newline
$t_1 + \frac{x}{r_1} + 2l + i + s \approx t_2 - \frac{x}{r_2} + s$
\newline
The $l$ in the above equation represents the cost of sending a message to the server alerting that work has finished, and then the server responding with either a $nack$ or $steal$ message.
\newline
If updates are used then the cost of both of those messages, as well as the cost of reading the images to be stolen, will in the average case reduce to no cost. Using updates allows the server to co-ordinate workers before they have finished their initial batches, and therefore no extra overhead results. Note that the $s$ value can also be ignored, as it is constant for both the victim of work stealing, as well as the worker stealing work.
\newline
The reformulated approximation to account for updates is then.
\newline
$t_1 + \frac{x}{r_1} \approx t_2 - \frac{x}{r_2}$
\newline
$t_2$ is not a fixed value as it will occur in the future and therefore a prediction of its value shall be used instead. At $t_1$ the slower worker has $m$ images left to process this round, and would have $m -x $ images left after work stealing. Therefore an estimate for $t_2$ is $t_1 + \frac{m-x}{r_2}$, using a linear regression based on the training rate achieved by the slower worker this round. As mentioned previously, the assumption used is that in the average case that workers maintain a constant training rate per round.
\newline
The approximation can therefore be rewritten as.
\newline
$t_1 + \frac{x}{r_1} \approx t_1 + \frac{m-x}{r_2} - \frac{x}{r_2}$
\newline
A rearrangement of this approximation to solve for $x$ leads to.
\newline
$x \approx \frac{r_1m}{r_2 + 2r_1}$
\newline
$m$ can also be approximated as the exact value at $t_1$ is not known. All workers begin each round with $I$ images to process, and using another linear regression on the training rate the following approximation can occur.
\newline
$m \approx I - t_1r_2$
\newline
Which leads to a final approximation of x.
\newline
$x \approx \frac{r_1(I - t_1r_2)}{r_2 + 2r_1}$
\newline
These values are all known by the server which allows it to make a high quality prediction for whether it would be beneficial to steal work this round. It is a deliberate choice to calculate these values on a per-round basis to provide the greatest level of dynamic work stealing, and also the overhead of such a calculation is minimal. The result of this approximation is defined as $minimumStealingAmount$ in Algorithm \ref{CentralisedAlgorithmServer}.

\subsection{Decentralised Model}
The decentralised model operates on a worker-to-worker level for arranging whether work-stealing shall occur. It uses many of the same techniques as the centralised model, but the parameter server is used solely as a store for the machine learning model.
\newline
The benefit of the system is that the logic for determining whether to steal work is co-located with the state responsible for tracking worker progress. The initial reasoning for the efficacy of the decentralised model was that it would attempt to reduce overhead by removing additional messages.

\subsubsection{Decentralised Algorithm for Workers}

Algorithm \ref{DecentralisedAlgorithmWorkerBeginRound} details how workers handle the beginning of each round. As in the centralised version workers receive a \say{beginRound} message from the server to indicate that they should commence training on the next minibatch of images.
\newline
Using the same technique as the centralised version, workers will send a message when they approach the final message in a batch, rather than waiting to complete the minibatch. This is done as a performance improvement, and Figure \ref{RoundDegradation} shows that transient events do not tend to affect workers on a per-round basis, which leads to the simplifying assumption that each round displays a constant training rate for each worker.

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwProg{Fn}{Function}{}{}
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Input{The size of each minibatch $n$}
 \Input{The number of workers $w$}
 \Input{The number of images to be processed before sending message $threshold$}
 \BlankLine
 \Fn{on("beginRound", n, w, threshold)}{
   $sentRemaining \longleftarrow false$\;
   $receivedRemaining \longleftarrow false$\;
   $responsesOutstanding \longleftarrow w - 1$\;
   $N \longleftarrow nextMiniBatch()$\;
   \For{$i \gets 1$ \KwTo $n$}{
      \If{$i = threshold \land \neg receivedRemaining$}{
        $send("remaining")$\;
      }
      $train(N[i])$\;
   }
   \If{$\neg sentRemaining$}{
     $sendResultToServer()$\;
   }
 }
 \caption{Centralised Algorithm executed by workers}
 \label{DecentralisedAlgorithmWorkerBeginRound}
\end{algorithm}
\DecMargin{1em}
\medskip

The logic to handle a minibatch is very similar to the decentralised system. If a worker reaches the threshold required to commence the work-stealing process and it has not received messages from other workers this round it will broadcast a \say{remaining} message. This broadcast will also set an internal boolean variable \say{sentRemaining}. After training completes workers will either send their models to the server if they were not the first to attempt work stealing, otherwise they will not.
\newline
There are guarantees imposed on further steps in the algorithm which ensure that if a worker were to send a \say{remaining} message that the local model would also be sent to the server. Thus, per round, it can be seen that all workers will send their local models, ensuring liveness in the system.

\subsubsection{Minimum Sufficient Work Stealing Value}
Similar to the centralised model there exists a minimum value for which work stealing will be beneficial. If the model in which messages are exchanged prior to round completion is considered then the resulting approximation for the number of images to steal, x,  is identical.
\newline
$x \approx \frac{r_1(I - t_1r_2)}{r_2 + 2r_1}$
\newline
Where $r_1$ and $r_2$ are the training rates of the fastest and slowest workers respectively. The slower worker, upon receiving the message will be able to determine the faster worker's rate by accounting for a slight message delay, and it will know its own rate exactly.

\subsubsection{Decentralised Algorithm Work Stealing Protocol}

Algorithms \ref{DecentralisedAlgorithmWorkerRemaining} and \ref{DecentralisedAlgorithmWorkerResponse} detail the two-way protocol which facilitates the stealing of work. In Algorithm \ref{DecentralisedAlgorithmWorkerBeginRound} it can be seen in which circumstances a worker will send a \say{remaining} message, which corresponds to the initial step of the work stealing protocol.
\newline
Upon the broadcast of the \say{remaining} message, the fastest worker awaits the arrival of a response from all other workers. This is done to impose a logical ordering on work that is stolen to ensure that a new round will not begin until all work from the previous round has completed. The remaining workers can respond with two messages types, a \say{nack} to indicate that they do not have sufficient work to steal, and a \say{remainingResponse} which instructs the fastest worker to read a number of images at a certain offset.
\newline


\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwProg{Fn}{Function}{}{}
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Fn{on("remaining")}{
  \If{$receivedRemaining \lor sentRemaining$}{
    $reply("nack")$\;
  }
  $receivedRemaining \longleftarrow true$\;
  \uIf{$remainingImages > minimum$}{
    $amount \longleftarrow determineAmountToSteal(remainingImages)$\;
    $offset \longleftarrow calculateOffset(amount)$\;
    $remainingImages \longleftarrow remainingImages - amount$\;
    $skipImages(amount)$\;
    $reply("remainingResponse", amount ,offset)$\;
  }
  \Else{
    $reply("nack")$\;
  }
 }
 \caption{Handle initial outgoing message from fastest worker}
 \label{DecentralisedAlgorithmWorkerRemaining}
 \end{algorithm}
 \DecMargin{1em}
 \medskip

Similar to the centralised model, if a worker has at least a $minimum$ number of images remaining then it will respond with a \say{remainingResponse} message, and skip the images that will be stolen. If the minimum is not met, then a \say{nack} is sent to alert the faster worker that no work stealing shall occur.

\IncMargin{1em}
\begin{algorithm}[H]
 \SetKwProg{Fn}{Function}{}{}
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \Fn{on("nack")}{
   $reponsesOutstanding \longleftarrow responsesOutstanding - 1$\;
   \If{$responsesOutstanding = 0$}{
     $sendResultToServer()$\;
   }
 }
 \BlankLine
 \BlankLine
 \setcounter{AlgoLine}{0}
 \Input{The amount of work to steal $count$}
 \Input{The offset at which to read images from $offset$}
 \BlankLine
 \Fn{on("remainingResponse", count, offset)}{
   $reponsesOutstanding \longleftarrow responsesOutstanding - 1$\;
   $stolenImages \longleftarrow readImages(count, offset)$\;
   $awaitTraining()$\;
   \For{$i \gets 1$ \KwTo $count$}{
     $train(stolenImages[i])$\;
   }
   \If{$responsesOutstanding = 0$}{
     $sendResultToServer()$\;
   }
 }
 \caption{Handle possible responses from slower workers}
 \label{DecentralisedAlgorithmWorkerResponse}
\end{algorithm}
\DecMargin{1em}
\medskip

It is guaranteed that the fastest worker will send its local model to the server regardless of the responses received, through the use of the \say{responsesOutstanding} internal state tracker.
\newline
If the fastest worker has work to steal, it will read the images at the appropriate offset and augment its local model. The function $awaitTraining()$ is used as message handling occurs concurrently with all processing. This includes if multiple work stealing messages are received and the fastest worker must train on stolen work multiple times.

\subsubsection{Disadvantages of Decentralised Algorithm}
A major concern for the decentralised model is the lack of co-ordination between the slower workers. When using the centralised model, it can be ensured that only a single batch of stolen work is sent to the fastest worker as well as only one worker stealing work. This section shall contain an analysis of these problems, as well as mitigations in application.
\newline
Message transmissions incur a physical time delay. Figure \ref{messageInterleave} visualises how multiple wokers could believe that they are the fastest worker. Based on the use of this work stealing algorithm in practise it is very common that at least two workers send \say{remaining} messages.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Explanation of how physical timestamps cannot be used to represent causality.}
  \label{messageInterleave}
\end{figure}

The following two rules define the decentralised work stealing protocol in the event of multiple workers considering themselves to be the fastest.

\begin{enumerate}
\item No worker shall have its work stolen more than once.
\item All work must be trained on.
\end{enumerate}

It is for this reason that Algorithm \ref{DecentralisedAlgorithmWorkerRemaining} contains the $receivedRemaining \lor sentRemaining$ statement. If a worker has already received a \say{remaining} message then it can have responded with either a \say{nack}, in which case any future messages should return \say{nack} as the number of images remaining decreases monotonically, or it will have responded with a \say{remainingResponse} in which case a \say{nack} should be returned to oblige with the first rule.
\newline
If message complexity is considered then the worst case is when all workers consider themselves to the fastest worker. With $n$ workers, and each worker anticipating $n-1$ responses, the message complexity would be $\mathcal{O}(n^2)$. If all workers were to operate at the same rate then this would also turn into the average case.
\newline
The aim is to therefore prevent all workers sending \say{remaining} messages at the same time regardless of rate of training. A solution to this is to add some randomness into the threshold value used to determine whether or not a worker should send a \say{remaining} message. It is important that such a skew is small enough to not cause stragglers to possibly report themselves as the fastest worker, and to also prevent the fastest workers from fetching work too late. The value used should also be randomly generated on a per-round basis, otherwise it may result in a single worker continuously stealing work even if another worker were further ahead.
\newline
A value of $0.1*miniBatchSize$ has found to be useful, and Figure \ref{skewValue} presents an analysis on the range of possible values used, including no skew.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Explanation of how physical timestamps cannot be used to represent causality.}
  \label{skewValue}
\end{figure}

Even when using the skew value to offset workers, the worst case in terms of message complexity still remains $\mathcal{O}(n^2)$, however, the average case is now $\mathcal{O}(n)$. This represents a large decrease in the number of messages sent, and is a requirement for the model to scale well to many workers.

The other main problem with the decentralised model is the possibility of many workers sending \say{remainingResponse} messages to the fastest worker. This represents the largest flaw with the decentralised model.

\newpage

\section{Design and Implementation}

The project developed on top of two existing bodies of work: SEEP, a distributed processing platform, and Ako, a machine learning framework built on top of SEEP. The choice was made as I had experience developing with SEEP and was comfortable working with its API, and Ako was a well-structured, easily-modifiable codebase which allowed someone with little machine learning experience to make major modifications and understand the impact of those changes.

\subsection{Machine Learning Framework}

Other machine learning frameworks were considered, such as Google's Tensorflow, Caffe, Theano, and deeplearning4j. Major concerns were the ability to work with a distributed system, as the basis for this project was mitigating the impact of stragglers in a multi-worker environment. This meant the exclusion of Theano, and also Tensorflow as a distributed version of the framework was not released until some time after beginning the project. Of the remaining frameworks Caffe is C++ based, while deeplearning4j and Ako are Java based. Based on personal programming experience, the choice was made to develop using Java, as I have a much greater level of experience and the language constructs for the algorithms are much simpler to use in Java. This removed Caffe as an option. Ako is a simpler framework than deeplearning4j and is designed to be less general, and as a result is less complex. While this would limit its viability in a commercial setting, as a baseline from which to develop on it was a much better choice than deeplearning4j. The modified version of Ako that has been developed also makes heavy use Java 8 Lambdas to reduce boilerplate code that adds additional visual clutter.
\newline
\newline
Note that no part of this project is dependent upon any features provided by SEEP or Ako, and that the algorithms presented would function just as well in any distributed framework.

\subsection{Architecture}
SEEP requires that computations are expressed as directed graphs where vertices represent tasks, and edges represent the ability for tasks to communicate. Ako on SEEP requires the following tasks:

\begin{enumerate}
\item Source for reading data
\item Sink for emitting the results of training
\item Parameter Server for maintaining global state
\end{enumerate}

Ako also has the ability to configure a variable number of workers to increase performance, and each worker is modelled as a separate task.
\newline
\newline
SEEP tasks are represented as Java objects which implement the \textbf{SeepTask} interface. The most important methods that must be implemented in the interface are \textbf{setUp}, used as a run-time constructor, and \textbf{processData}, used to handle incoming messages. Messages in SEEP are required to conform to a schema imposed when constructing the edges in the directed graph. Schemas in SEEP are a set of key-value pairs, with keys being strings representing the name of the value, and the value being of a fixed type. Listing \ref{AkoSchema} is the schema used in Ako for inter-task communication.

\begin{lstlisting}[caption={The SEEP schema used for Ako},label=AkoSchema]
Schema schema = Schema.SchemaBuilder.getInstance()
                .newField(Type.STRING, "task")
                .newField(Type.BYTES, "content")
                .build();
}
\end{lstlisting}

The \say{task} field is used to identify the type of message, such as: \say{beginRound}, or \say{remaining}, and content is a byte array. The rigidity of the schema imposed some issues, such as requiring to convert Java integers into a byte array representation during transmission even though SEEP does contain the ability to handle integers in a schema. However, this issue was solved using the \textbf{ByteBuffer} class in Java.
\newline
\newline
Figure \ref{AkoGraph} is the SEEP graph when Ako is using two workers. More workers were typically used, however the graph complexity increases quadratically with the number of workers and does not provide any more detail.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{The SEEP directed graph used by Ako with two workers}
  \label{AkoGraph}
\end{figure}

All components message-driven\footnote{https://en.wikipedia.org/wiki/Event-driven\_architecture} and every action performed is in response to an incoming message. The control can be represented by Figure \ref{ControlFlow} showing a simplification of the relations between the Source, Parameter Server, and Workers.

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{todo}
  \caption[]{Control Flow between components}
  \label{ControlFlow}
\end{figure}

\subsection{Tasks}

Each task in the SEEP graph represents some either a required SEEP construct, or a component of the machine learning program.

\subsubsection{Source and Sink}

SEEP requires the presence of a Source task and a Sink task, which both implement the \textbf{SeepTask} interface. The Sink in Ako is not used, as all communication exists solely between the workers and parameter server during training.
\newline
\newline
In Ako, the state of the parameter server can be saved to a file, and loaded during another training run so as to not commence from a completely untrained model each time. The Source is responsible for loading the saved model from a file and sending to both the workers and parameter server.

\subsubsection{Parameter Server}

The Parameter Server is responsible for receiving and applying updates to the global model from workers, and also to co-ordinate the rounds of minibatches that will be processed. It's functionality is stored in the \textbf{ParamServer} class.
\newline
\newline
All classes in Ako use the Java Kryo library \cite{kryo} for object serialisation and all serialised objects are represented as byte arrays to conform to the SEEP schema. After the update from the worker has been deserialised, it is used to augment the global model. Once the number of updates equals the number of workers then a round is complete, and the parameter server will send a message to alert the workers to commence a new round.
\newline
\newline
In order to prevent the Parameter Server from becoming a bottleneck in the system, messages are handled concurrently. Under the original Ako codebase, the Parameter Server would process messages sequentially, however, as performance is a concern the handling was changed to be in parallel. Listing \ref{ParameterServerParallel} contains the source code for the Parameter Server model update message handler.

\begin{lstlisting}[caption={Parameter Server concurrent message handling},label=ParameterServerParallel]
executor.submit(() -> {
	int sid = dataTuple.getStreamId();
	byte[] bytes = dataTuple.getBytes("content");

    ModelNN gradOnlyModel = useSpecificKryoToDeserialiseObject(sid, bytes);

    //Apply gradient to the neural-net model in this param server
    applyAccumGradFromAnotherModelToMainModelDirectlyWithoutResetAccum(gradOnlyModel);
    counter.incrementAndGet();

    if (counter.compareAndSet(numReplica, 0)) {
        //Extract only the weight from the current neural-net model state
        ModelNN weightOnlyModel = ModelNN.produceModelNNWithWeightParameterPartOnly(getCurrentModelNNInstance());

        byte[] weightOnlyModel_bytes = useSpecificKryoToSerialiseObject(sid, weightOnlyModel);

        byte[] newGlobalModel = OTuple.create(schema,
                new String[]{"task", "content"},
                new Object[]{BEGIN_ROUND, weightOnlyModel_bytes});

        for (int i : rep_sids) {
            api.sendToStreamId(i, newGlobalModel);
        }
        round++;
    }
});
\end{lstlisting}

The executor variable is a Java ExecutorService, a concurrency object in which tasks can be submitted and executed in a pool of threads. The thread pool used contains the same number of threads as the number of workers. With a very large number of workers this option would cause degraded performance as the amount of work would exceed the resources available, however it scales well up to the number of cores in a machine.
\newline
\newline
This code block is located within the method responsible for handling incoming messages. In Java method parameters are thread local, meaning that each thread will receive its own copy. The \say{dataTuple} parameter referenced above is the parameter containing the message contents, but it can be safely passed to another thread in the ExecutorService because no other thread could have a reference to it. The code block also makes heavy use of the Java AtomicInteger object type. This is a thread-safe atomically incrementable integer which allows for low-overhead synchronisation and the ability to track the number of responses received from workers. The AtomicInteger in this case also ensures that all local updates have taken place before sending a \say{BEGIN\_ROUND} message, preventing the case where workers do not receive the freshest copy of the global state at the beginning of a round.

\begin{figure}[H]
  \centering
  \includegraphics[width=2.5in]{parameterserveruml}
  \caption[]{UML diagram of the parameter server class}
  \label{ParameterServerUML}
\end{figure}

\subsubsection{Worker}

The workers are responsible for processing of images, augmenting their local models, and after finishing a minibatch, to send them to the parameter server. The workers were initially designed to only handle one type of message per round, so a significant effort was made to convert the system into an asynchronous model. This applies to both the centralised and decentralised models, as both have message types to respond to while training.
\newline
\newline
Workers make heavy use of Java concurrency primitives to ensure thread safety and no data races. Languages such as Rust are able to remove the possibility of data races with the compiler, and this would have been an incredibly useful aide during programming as subtle bugs were common. Java contains a concurrency tool \textbf{Future} which represents the result of an asynchronous computation. It was used to ensure that a single worker would not block on the loading of images that it had stolen. The \textit{synchronized} keyword also made reasoning about control flow much easier as it guarantees that for a single instantiation of an object that a certain method cannot be called concurrently with any other thread. An ideal use case for this was the \textbf{train()} method, where training on stolen work should be delayed until the initial training is complete, but should also execute as soon after as possible.
\newline
\newline
Advice for concurrent programming is to avoid mutable and global state. If the messages being passed around contain all that is needed to execute the algorithm then there is no chance of data races. Unfortunately, in order for the message size to not have a significantly adverse affect on performance, they can only contain references and not values. Global state is also required as the messaging framework provided by SEEP is simply a method call, and Java object fields are required to store state between messages. The problem domain is also one that requires mutable state. The models are too large to simply create a new, immutable version on every update, and doing so would impose a heavy performance penalty for creating and garbage collecting so many objects.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{workeruml}
  \caption[]{UML diagram of the worker class}
  \label{WorkerUML}
\end{figure}

\newpage

\section{Evaluation}

The primary objective of this project was to determine whether an algorithm could be developed that would mitigate the impact of stragglers in a machine learning framework. The objective metric is therefore a comparison in the rate of progress that is made when stragglers are present with and without the use of work stealing. In order for the approach to be valid, the accuracy of the machine learning model must not be negatively affected by work stealing.
\newline
\newline
There are several different parameters that can effect the efficacy of work stealing, and evaluations will vary these parameters to determine their effect.

\begin{enumerate}
\item Number of workers
\item Number of cores per worker
\item Number of stragglers
\item Minibatch size
\item Centralised vs Decentralised algorithm
\end{enumerate}

In order to provide consistent results, stragglers will be created by either using less powerful machines that  take longer to train per round, or by introducing a CPU-intensive workload intermittently on an equally powerful machine.

\subsection{Expected Results}
Prior to running any evaluation experiments, hypotheses will be made, and the objective results will validate or reject these hypotheses.

\begin{enumerate}
\item More workers decreases training time, but at a sub-linear rate.
\item Increased cores will improve performance per worker at a linear rate.
\item The number of stragglers will severely impact the efficacy of the algorithms.
\item A larger minibatch size will correlate well with algorithm efficacy, but will result in worse accuracy.
\item The Centralised algorithm will scale better than the Decentralised.
\end{enumerate}

\newpage

\section{Conclusions and Future Work}

\newpage

\bibliographystyle{plain}
\bibliography{James-DavidCarr-MastersReport}

\end{document}










































